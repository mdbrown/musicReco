mycox<- coxph(Surv(Yrrand_LROutcome, LiverRelatedOutcome) ~ ns(meas.time, df = 3) + age + female +
PLT.blup +# PLT.bi.s_slope +
ASTALT.blup +
#  ASTALT.bi.s_slope +
BIL.blup +  #BIL.bi.s_slope +
ALB.blup,  #ALB.bi.s_slope,
data = timedat.cc)
library(pander)
library(ggplot2)
library(mi)
library(reshape)
library(stringr)
library(RColorBrewer)
library(nlme)
library(lme4)
library(survival)
timedat.cc <- timedat[complete.cases(timedat),]
#fit my cox model
mycox<- coxph(Surv(Yrrand_LROutcome, LiverRelatedOutcome) ~ ns(meas.time, df = 3) + age + female +
PLT.blup +# PLT.bi.s_slope +
ASTALT.blup +
#  ASTALT.bi.s_slope +
BIL.blup +  #BIL.bi.s_slope +
ALB.blup,  #ALB.bi.s_slope,
data = timedat.cc)
source("../Rscripts/fitting-mm.r")
source("../Rscripts//stats-mm.r")
data.s2 <- NULL
timedat.lt2 <- timedat.cc[timedat.cc$meas.time <=2, ]
for(id in unique(timedat$sub.id.consec)){
tmp <- timedat.lt2[timedat.lt2$sub.id.consec==id,]
data.s2 <- rbind(data.s2, tmp[which.max(tmp$meas.time),])
}
tau = 2; si = 2;
ti = tau + si
dc = list("pred.time" = c(tau), "si" = si, "ti" = ti)
risk.pc <- get.pc.risk(dc = dc, fit = mycox, data.s = data.s2)
risk.baseline.discrete <- get.pc.risk(dc = dc, fit = model.2B, data.s = subdat_dc)
risk.baseline.cont <- get.pc.risk(dc = dc, fit = model.2A, data.s = subdat)
pc.tiles <- c(0,quantile(risk.pc, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.dc.tiles <- c(0,quantile(risk.baseline.discrete, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.cont.tiles <- c(0,quantile(risk.baseline.cont, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
pc.cut <- cut(risk.pc, pc.tiles)
bs.dc.cut <- cut(risk.baseline.discrete, bs.dc.tiles)
bs.cont.cut <- cut(risk.baseline.cont, bs.cont.tiles)
data.s2$groups = pc.cut
subdat_dc$groups = bs.dc.cut
subdat$groups = bs.cont.cut
#observed event rates from kaplan meier
obs.pc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = data.s2, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.dc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat_dc, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.cont <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat, se.fit = FALSE ), times = dc$pred.time)$surv
obs.exp.dat <- NULL
#average predicted risks by group
##pc
pc <- data.frame("risk" = t(risk.pc), "group" = pc.cut)
pred.pc <- aggregate(risk~group, data = pc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- data.frame("observed" = obs.pc, "predicted" = pred.pc, "model" = "pc")
#discrete
bs.dc <- data.frame("risk" = t(risk.baseline.discrete), "group" = bs.dc.cut)
pred.bs.dc <- aggregate(risk~group, data = bs.dc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.dc, "predicted" = pred.bs.dc, "model" = "baseline discrete"))
#continous
bs.cont <- data.frame("risk" = t(risk.baseline.cont), "group" = bs.cont.cut)
pred.bs.cont <- aggregate(risk~group, data = bs.cont, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.cont, "predicted" = pred.bs.cont, "model" = "baseline continuous"))
ggplot(obs.exp.dat, aes(observed, predicted, colour = model)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(a=0,b=1)
load("../Data/timedat_marker_scaled.Rdata")
timedat.cc <- timedat[complete.cases(timedat),]
#fit my cox model
mycox<- coxph(Surv(Yrrand_LROutcome, LiverRelatedOutcome) ~ ns(meas.time, df = 3) + age + female +
PLT.blup +# PLT.bi.s_slope +
ASTALT.blup +
#  ASTALT.bi.s_slope +
BIL.blup +  #BIL.bi.s_slope +
ALB.blup,  #ALB.bi.s_slope,
data = timedat.cc)
#data set with only time points at time month 24
data.s2 <- NULL
timedat.lt2 <- timedat.cc[timedat.cc$meas.time <=2, ]
for(id in unique(timedat$sub.id.consec)){
tmp <- timedat.lt2[timedat.lt2$sub.id.consec==id,]
data.s2 <- rbind(data.s2, tmp[which.max(tmp$meas.time),])
print(id)
}
load("baselineModels.Rdata")
tau = 2; si = 2;
ti = tau + si
dc = list("pred.time" = c(tau), "si" = si, "ti" = ti)
risk.pc <- get.pc.risk(dc = dc, fit = mycox, data.s = data.s2)
risk.baseline.discrete <- get.pc.risk(dc = dc, fit = model.2B, data.s = subdat_dc)
risk.baseline.cont <- get.pc.risk(dc = dc, fit = model.2A, data.s = subdat)
pc.tiles <- c(0,quantile(risk.pc, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.dc.tiles <- c(0,quantile(risk.baseline.discrete, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.cont.tiles <- c(0,quantile(risk.baseline.cont, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
pc.cut <- cut(risk.pc, pc.tiles)
bs.dc.cut <- cut(risk.baseline.discrete, bs.dc.tiles)
bs.cont.cut <- cut(risk.baseline.cont, bs.cont.tiles)
data.s2$groups = pc.cut
subdat_dc$groups = bs.dc.cut
subdat$groups = bs.cont.cut
#observed event rates from kaplan meier
obs.pc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = data.s2, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.dc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat_dc, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.cont <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat, se.fit = FALSE ), times = dc$pred.time)$surv
obs.exp.dat <- NULL
#average predicted risks by group
##pc
pc <- data.frame("risk" = t(risk.pc), "group" = pc.cut)
pred.pc <- aggregate(risk~group, data = pc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- data.frame("observed" = obs.pc, "predicted" = pred.pc, "model" = "pc")
#discrete
bs.dc <- data.frame("risk" = t(risk.baseline.discrete), "group" = bs.dc.cut)
pred.bs.dc <- aggregate(risk~group, data = bs.dc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.dc, "predicted" = pred.bs.dc, "model" = "baseline discrete"))
#continous
bs.cont <- data.frame("risk" = t(risk.baseline.cont), "group" = bs.cont.cut)
pred.bs.cont <- aggregate(risk~group, data = bs.cont, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.cont, "predicted" = pred.bs.cont, "model" = "baseline continuous"))
ggplot(obs.exp.dat, aes(observed, predicted, colour = model)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(a=0,b=1)
names(data.s2)[c(2,6)] = c("t.star", "status")
subdat_dc$meas.time = 2.0
subdat_dc$time = subdat_dc$Yrrand_LROutcome  + 2.0
subdat_dc$t.star = subdat_dc$Yrrand_LROutcome
subdat_dc$status = subdat_dc$LiverRelatedOutcome
risk.baseline.discrete <- get.pc.risk(dc = dc, fit = model.2B, data.s = subdat_dc)
stats.baseline_discrete <- get.stats(dc = dc, data.s = subdat_dc, risk = risk.baseline.discrete)
subdat$meas.time = 2.0
subdat$time = subdat_dc$Yrrand_LROutcome + 2.0
subdat$t.star = subdat_dc$Yrrand_LROutcome
subdat$status = subdat_dc$LiverRelatedOutcome
risk.baseline.cont <- get.pc.risk(dc = dc, fit = model.2A, data.s = subdat)
stats.baseline_cont <- get.stats(dc = dc, data.s = subdat, risk = risk.baseline.cont)
stats <- data.frame("PC" = stats.pc, "baseline.continuous" = stats.baseline_cont, "baseline.discrete" = stats.baseline_discrete)
risk.baseline.discrete <- get.pc.risk(dc = dc, fit = model.2B, data.s = subdat_dc)
risk.baseline.cont <- get.pc.risk(dc = dc, fit = model.2A, data.s = subdat)
pc.tiles <- c(0,quantile(risk.pc, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.dc.tiles <- c(0,quantile(risk.baseline.discrete, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.cont.tiles <- c(0,quantile(risk.baseline.cont, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
pc.cut <- cut(risk.pc, pc.tiles)
bs.dc.cut <- cut(risk.baseline.discrete, bs.dc.tiles)
bs.cont.cut <- cut(risk.baseline.cont, bs.cont.tiles)
data.s2$groups = pc.cut
subdat_dc$groups = bs.dc.cut
subdat$groups = bs.cont.cut
#observed event rates from kaplan meier
obs.pc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = data.s2, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.dc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat_dc, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.cont <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat, se.fit = FALSE ), times = dc$pred.time)$surv
obs.exp.dat <- NULL
#average predicted risks by group
##pc
pc <- data.frame("risk" = t(risk.pc), "group" = pc.cut)
pred.pc <- aggregate(risk~group, data = pc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- data.frame("observed" = obs.pc, "predicted" = pred.pc, "model" = "pc")
#discrete
bs.dc <- data.frame("risk" = t(risk.baseline.discrete), "group" = bs.dc.cut)
pred.bs.dc <- aggregate(risk~group, data = bs.dc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.dc, "predicted" = pred.bs.dc, "model" = "baseline discrete"))
#continous
bs.cont <- data.frame("risk" = t(risk.baseline.cont), "group" = bs.cont.cut)
pred.bs.cont <- aggregate(risk~group, data = bs.cont, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.cont, "predicted" = pred.bs.cont, "model" = "baseline continuous"))
ggplot(obs.exp.dat, aes(observed, predicted, colour = model)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(a=0,b=1)
install.packages("survAccuracyMeasures")
getwd()
setwd("Z:/Yingye/applied/HALT-C/Analysis")
setwd("~/")
getwd()
install.packages("survAccuracyMeasures")
library(survAccuracyMeasures)
?survAM.estimate
exp(.06)
exp(-.04)
exp(.05)
exp(.02)
exp(74)
exp(.74)
exp(.09)
exp(-.055)
exp(0.41)
exp(.54)
exp(.13)
exp(-2.98)
exp(-.38)
exp(0.9665)
exp(0.0665)
exp(-0.06)
exp(0.015)
exp(0.08)
log(1.88)
exp(1.88)
rm(list = ls())
load(survAccuracyMeasures)
library(survAccuracyMeasures)
data(SimData)
SimData$Y.2 <- rnorm(nrow(SimData))
head(SimData)
my.coxph <- coxph(Surv(survTime, status) ~ Y + Y.2, data = SimData)
mycoxph
my.coxph
?predict
predict.coxph
?predict.coxph
linear.combo <- predict(my.coxph, type = "lp")
?survAM.estimate
survAM.estimate(time = survTime, event = status, marker = linear.combo, predict.time = 2, bootstraps = 0)
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData, predict.time = 2, bootstraps = 0)
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "cox", se.method = "asymptotic")
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "Cox", se.method = "asymptotic")
predict.time = 2, estimation.method = "IPW", se.method = "asymptotic")
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "IPW", se.method = "asymptotic")
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "IPW")
n = nrow(SimData)
estimates <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "Cox", se.method = "asymptotic")
estimates
names(estimates)
names(estimates$estimates)
measures <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "Cox", se.method = "asymptotic")
#I am not confident in the asymptotic
B = 500
bootstrapEstimates <- data.frame(matrix(nrow = B, ncol =  6)); names(boostrapEstimates) = names(measures$estimates)
bootstrapEstimates <- data.frame(matrix(nrow = B, ncol =  6));
names(boostrapEstimates) = names(measures$estimates)
names(bootstrapEstimates) = names(measures$estimates)
measures
B = 500
set.seed(12321)
#matrix to hold bootstrapped estimates
bootstrapEstimates <- data.frame(matrix(nrow = B, ncol =  6));
names(bootstrapEstimates) = names(measures$estimates)
#begin bootstrapping
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootData,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootData,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
#begin bootstrapping
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData, type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
#begin bootstrapping
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(Surv(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(Surv(survTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
prediction.time = 2
my.cutpoint = 0
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(Surv(survTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
estimated.se <- apply(boostrapEstimates, 2, sd)
estimated.se <- apply(bootstrapEstimates, 2, sd)
estimated.se
round(estimated.se, 3)
measures
qnorm(1-alpha/2)
alpha = .05
qnorm(1-alpha/2)
measures - qnorm(1-alpha/2)*estimated.se
qnorm(1-alpha/2)*estimated.se
measures$estimates - qnorm(1-alpha/2)*estimated.se
3^5
3^5*3
1015/3^5
2015/3^5
2015/3*5
2015/(3*5)
3^5
install.packages("DT")
library(devtools)
devtools::install_github("rstudio/DT")
library(DT)
datatable(iris)
library(htmlWidgets)
library(htmlwidgets)
library(htmlwidgets)
library(DT)
datatable(iris)
devtools::install_github("rstudio/htmlwidgets")
devtools::install_github("htmlwidgets")
devtools::install_github("htmlwidgets", user = "rstudio")
install_github('rstudio/rmarkdown')
datatable(cars)
datatable(cars, rownames=TRUE)
cars
head(cars)
datatable(iris, rownames=TRUE)
datatable(iris, rownames=FALSE)
install_github("slidify")
install_github("slidify", "ramnathv")
install_github("slidifyLibraries", "ramnathv")")
)))
""
install_github("slidifyLibraries", "ramnathv")
install_github("knitr")
install.packages("knitr")
library(knitr)
library("knitr")
library(ggvis)
install_github("ggvis")
install_github("ggvis", "rstudio")
pressure %>% ggvis(~temperature,~pressure) %>%
layer_points() %>%
layer_lines()
library(ggvis)
install.packages("lazyeval")
install_github("lazyeval")
library('devtools')
library("devtools")
install.packates("devtools")
install.packages("devtools")
install.packages("ggplot2")
library(devtools)
install_github("ggvis", "rstudio")
install_github("rstudio/DT")
install_github("rstudio/rmarkdown")
library(DT)
library(knitr)
library(ggvis)
pressure %>%
ggvis(x = ~temperature, y = ~pressure) %>%
layer_bars()
install_github("rstudio/knitr")
install.packages("knitr")
install.packages("knitr")
library(knitr)
ggvis:::vega_file
ggvis:::vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
ggvis::vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
ggvis:::vega_file
ggvis:::vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
install.packages("rCharts")
install_github('ramnathv/rCharts')
library(devtools)
install_github('ramnathv/rCharts')
10099*1.5
10099*.015
install.packages("survival")
install.packages("plyr")
source("../R//mapFunctions.R")
library(dplyr)
setwd("~/GitHub/MusicRec/Analysis")
source("../R//mapFunctions.R")
library(dplyr)
library(data.table)
load("../Data/train_trips.Rdata") #loads trainTrips.dt data.table of traintrips.txt
setkey(trainTrips.dt, song)
validTrips.known.dt <- fread("../Data/kaggle_visible_evaluation_triplets.txt")#fread("../Data/EvalDataYear1MSDWebsite/year1_valid_triplets_visible.txt") #
setnames(validTrips.known.dt, c("user", "song", "count"))
setkey(validTrips.known.dt, song)
trainTrips.dt <- fread("../Data/taste_profile_song_to_tracks.txt") #loads trainTrips.dt data.table of traintrips.txt
trainTrips.dt <- fread("../Data/train_triplets.txt") #loads trainTrips.dt data.table of traintrips.txt
