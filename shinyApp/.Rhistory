obs.pc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = data.s2, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.dc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat_dc, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.cont <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat, se.fit = FALSE ), times = dc$pred.time)$surv
obs.exp.dat <- NULL
#average predicted risks by group
##pc
pc <- data.frame("risk" = t(risk.pc), "group" = pc.cut)
pred.pc <- aggregate(risk~group, data = pc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- data.frame("observed" = obs.pc, "predicted" = pred.pc, "model" = "pc")
#discrete
bs.dc <- data.frame("risk" = t(risk.baseline.discrete), "group" = bs.dc.cut)
pred.bs.dc <- aggregate(risk~group, data = bs.dc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.dc, "predicted" = pred.bs.dc, "model" = "baseline discrete"))
#continous
bs.cont <- data.frame("risk" = t(risk.baseline.cont), "group" = bs.cont.cut)
pred.bs.cont <- aggregate(risk~group, data = bs.cont, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.cont, "predicted" = pred.bs.cont, "model" = "baseline continuous"))
ggplot(obs.exp.dat, aes(observed, predicted, colour = model)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(a=0,b=1)
names(data.s2)[c(2,6)] = c("t.star", "status")
subdat_dc$meas.time = 2.0
subdat_dc$time = subdat_dc$Yrrand_LROutcome  + 2.0
subdat_dc$t.star = subdat_dc$Yrrand_LROutcome
subdat_dc$status = subdat_dc$LiverRelatedOutcome
risk.baseline.discrete <- get.pc.risk(dc = dc, fit = model.2B, data.s = subdat_dc)
stats.baseline_discrete <- get.stats(dc = dc, data.s = subdat_dc, risk = risk.baseline.discrete)
subdat$meas.time = 2.0
subdat$time = subdat_dc$Yrrand_LROutcome + 2.0
subdat$t.star = subdat_dc$Yrrand_LROutcome
subdat$status = subdat_dc$LiverRelatedOutcome
risk.baseline.cont <- get.pc.risk(dc = dc, fit = model.2A, data.s = subdat)
stats.baseline_cont <- get.stats(dc = dc, data.s = subdat, risk = risk.baseline.cont)
stats <- data.frame("PC" = stats.pc, "baseline.continuous" = stats.baseline_cont, "baseline.discrete" = stats.baseline_discrete)
risk.baseline.discrete <- get.pc.risk(dc = dc, fit = model.2B, data.s = subdat_dc)
risk.baseline.cont <- get.pc.risk(dc = dc, fit = model.2A, data.s = subdat)
pc.tiles <- c(0,quantile(risk.pc, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.dc.tiles <- c(0,quantile(risk.baseline.discrete, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
bs.cont.tiles <- c(0,quantile(risk.baseline.cont, probs=c(.1, .2, .3, .4, .5, .6, .7, .8,.9)), 1)
pc.cut <- cut(risk.pc, pc.tiles)
bs.dc.cut <- cut(risk.baseline.discrete, bs.dc.tiles)
bs.cont.cut <- cut(risk.baseline.cont, bs.cont.tiles)
data.s2$groups = pc.cut
subdat_dc$groups = bs.dc.cut
subdat$groups = bs.cont.cut
#observed event rates from kaplan meier
obs.pc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = data.s2, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.dc <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat_dc, se.fit = FALSE ), times = dc$pred.time)$surv
obs.bs.cont <- 1- summary(survfit(Surv(t.star, status==1)~groups, data = subdat, se.fit = FALSE ), times = dc$pred.time)$surv
obs.exp.dat <- NULL
#average predicted risks by group
##pc
pc <- data.frame("risk" = t(risk.pc), "group" = pc.cut)
pred.pc <- aggregate(risk~group, data = pc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- data.frame("observed" = obs.pc, "predicted" = pred.pc, "model" = "pc")
#discrete
bs.dc <- data.frame("risk" = t(risk.baseline.discrete), "group" = bs.dc.cut)
pred.bs.dc <- aggregate(risk~group, data = bs.dc, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.dc, "predicted" = pred.bs.dc, "model" = "baseline discrete"))
#continous
bs.cont <- data.frame("risk" = t(risk.baseline.cont), "group" = bs.cont.cut)
pred.bs.cont <- aggregate(risk~group, data = bs.cont, FUN = "mean", na.rm = TRUE)$risk
obs.exp.dat <- rbind(obs.exp.dat,
data.frame("observed" = obs.bs.cont, "predicted" = pred.bs.cont, "model" = "baseline continuous"))
ggplot(obs.exp.dat, aes(observed, predicted, colour = model)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(a=0,b=1)
install.packages("survAccuracyMeasures")
getwd()
setwd("Z:/Yingye/applied/HALT-C/Analysis")
setwd("~/")
getwd()
install.packages("survAccuracyMeasures")
library(survAccuracyMeasures)
?survAM.estimate
exp(.06)
exp(-.04)
exp(.05)
exp(.02)
exp(74)
exp(.74)
exp(.09)
exp(-.055)
exp(0.41)
exp(.54)
exp(.13)
exp(-2.98)
exp(-.38)
exp(0.9665)
exp(0.0665)
exp(-0.06)
exp(0.015)
exp(0.08)
log(1.88)
exp(1.88)
rm(list = ls())
load(survAccuracyMeasures)
library(survAccuracyMeasures)
data(SimData)
SimData$Y.2 <- rnorm(nrow(SimData))
head(SimData)
my.coxph <- coxph(Surv(survTime, status) ~ Y + Y.2, data = SimData)
mycoxph
my.coxph
?predict
predict.coxph
?predict.coxph
linear.combo <- predict(my.coxph, type = "lp")
?survAM.estimate
survAM.estimate(time = survTime, event = status, marker = linear.combo, predict.time = 2, bootstraps = 0)
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData, predict.time = 2, bootstraps = 0)
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "cox", se.method = "asymptotic")
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "Cox", se.method = "asymptotic")
predict.time = 2, estimation.method = "IPW", se.method = "asymptotic")
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "IPW", se.method = "asymptotic")
survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "IPW")
n = nrow(SimData)
estimates <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "Cox", se.method = "asymptotic")
estimates
names(estimates)
names(estimates$estimates)
measures <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = SimData,
predict.time = 2, estimation.method = "Cox", se.method = "asymptotic")
#I am not confident in the asymptotic
B = 500
bootstrapEstimates <- data.frame(matrix(nrow = B, ncol =  6)); names(boostrapEstimates) = names(measures$estimates)
bootstrapEstimates <- data.frame(matrix(nrow = B, ncol =  6));
names(boostrapEstimates) = names(measures$estimates)
names(bootstrapEstimates) = names(measures$estimates)
measures
B = 500
set.seed(12321)
#matrix to hold bootstrapped estimates
bootstrapEstimates <- data.frame(matrix(nrow = B, ncol =  6));
names(bootstrapEstimates) = names(measures$estimates)
#begin bootstrapping
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootData,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootData,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
#begin bootstrapping
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(SurvTime, status) ~ Y + Y.2, data = bootData, type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
#begin bootstrapping
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(Surv(SurvTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(Surv(survTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
prediction.time = 2
my.cutpoint = 0
for(b in 1:B){
#obtain bootstrap sample
bootstrap.ind <- sample.int(n, replace = TRUE)
bootData <- SimData[bootstrap.ind,]
#fit the cox model using the bootstrap data, and get the linear predictors
bootData$linear.combo <- predict(coxph(Surv(survTime, status) ~ Y + Y.2, data = bootData), type = "lp")
#estimate the summary measures using survAM.estimate
bootstrapEstimates[b,] <- survAM.estimate(time = survTime, event = status, marker = linear.combo, data = bootData,
predict.time = prediction.time, marker.cutpoint = my.cutpoint,
estimation.method = "Cox", se.method = "asymptotic")$estimates
}
estimated.se <- apply(boostrapEstimates, 2, sd)
estimated.se <- apply(bootstrapEstimates, 2, sd)
estimated.se
round(estimated.se, 3)
measures
qnorm(1-alpha/2)
alpha = .05
qnorm(1-alpha/2)
measures - qnorm(1-alpha/2)*estimated.se
qnorm(1-alpha/2)*estimated.se
measures$estimates - qnorm(1-alpha/2)*estimated.se
3^5
3^5*3
1015/3^5
2015/3^5
2015/3*5
2015/(3*5)
3^5
install.packages("DT")
library(devtools)
devtools::install_github("rstudio/DT")
library(DT)
datatable(iris)
library(htmlWidgets)
library(htmlwidgets)
library(htmlwidgets)
library(DT)
datatable(iris)
devtools::install_github("rstudio/htmlwidgets")
devtools::install_github("htmlwidgets")
devtools::install_github("htmlwidgets", user = "rstudio")
install_github('rstudio/rmarkdown')
datatable(cars)
datatable(cars, rownames=TRUE)
cars
head(cars)
datatable(iris, rownames=TRUE)
datatable(iris, rownames=FALSE)
install_github("slidify")
install_github("slidify", "ramnathv")
install_github("slidifyLibraries", "ramnathv")")
)))
""
install_github("slidifyLibraries", "ramnathv")
install_github("knitr")
install.packages("knitr")
library(knitr)
library("knitr")
library(ggvis)
install_github("ggvis")
install_github("ggvis", "rstudio")
pressure %>% ggvis(~temperature,~pressure) %>%
layer_points() %>%
layer_lines()
library(ggvis)
install.packages("lazyeval")
install_github("lazyeval")
library('devtools')
library("devtools")
install.packates("devtools")
install.packages("devtools")
install.packages("ggplot2")
library(devtools)
install_github("ggvis", "rstudio")
install_github("rstudio/DT")
install_github("rstudio/rmarkdown")
library(DT)
library(knitr)
library(ggvis)
pressure %>%
ggvis(x = ~temperature, y = ~pressure) %>%
layer_bars()
install_github("rstudio/knitr")
install.packages("knitr")
install.packages("knitr")
library(knitr)
ggvis:::vega_file
ggvis:::vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
ggvis::vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
ggvis:::vega_file
ggvis:::vega_file <- function (vis, file = NULL, type = "png")
{
if (!(type %in% c("png", "svg")))
stop("type must be 'png' or 'svg'")
if (is.null(file)) {
file <- paste0("plot.", type)
message("Writing to file ", file)
}
temp_dir <- tempfile(pattern = "ggvis")
dir.create(temp_dir)
cmd <- paste0("vg2", type)
cmdsearch <- Sys.which(paste0(c("", "./bin/", "./node_modules/.bin/"),
cmd))
found_idx <- which(nzchar(cmdsearch))
if (length(found_idx) == 0)
stop("Conversion program ", cmd, "not found.")
cmd <- cmdsearch[min(found_idx)]
json_file <- file.path(temp_dir, "plot.json")
vega_json <- save_spec(vis, json_file)
on.exit(unlink(json_file))
system2(cmd, args = c(json_file, file), stdout=TRUE)
}
install.packages("rCharts")
install_github('ramnathv/rCharts')
library(devtools)
install_github('ramnathv/rCharts')
10099*1.5
10099*.015
install.packages("survival")
install.packages("plyr")
setwd("~/GitHub/MusicRec/shinyApp")
library(shiny)
install.packages("shiny")
librar(data.table)
library(datatable)
library(data.table)
songInfo.dt <- fread("../Data//unique_tracks.txt")
songInfo
songInfo.dt
songInfo.dt <- fread("../Data//unique_tracks.txt", sep = "<SEP>")
songInfo.dt <- fread("../Data//unique_tracks.txt", sep2= "<SEP>")
songInfo.dt
?fread
songInfo.dt <- fread("../Data//unique_tracks.txt", sep= "<SEP>")
songInfo.dt <- fread("../Data//unique_tracks.txt", sep= "<")
songInfo.dt
songInfo.dt <- fread("../Data//unique_tracks.txt", sep= "<SEP>")
?fread
songInfo.dt <- read.table("../Data//unique_tracks.txt", sep= "<SEP>")
songInfo.dt <- read.table("../Data//unique_tracks.txt", sep= " ")
songInfo.dt <- fread("../Data//unique_tracks.txt", sep= " ")
songInfo.dt
songInfo.dt <- fread("../Data//unique_tracks.txt", sep= "\")
)
""
]
\
\\
""
songInfo.dt <- fread("../Data//unique_tracks.txt", sep= "\\")
songInfo.dt <- fread("../Data//unique_tracks.txt", sep="\")
)
\""
""
songInfo.dt <- fread("../Data//unique_tracks.txt", sep="\ ")
songInfo.dt <- fread("../Data//unique_tracks.txt", sep="\\ ")
songInfo.dt <- fread("../Data//unique_tracks.txt", sep="\\")
songInfo.dt <- fread("../Data//unique_tracks.txt", sep="\\")
songInfo.dt
setnames(songInfo.dt, c("trackid", "songid", "artist", "song"))
setkey(songInfo.dt, songid)
trainTrips.dt <- fread("../Data/train_triplets.txt") #loads
setnames(trainTrips.dt, c("user", "song", "count"))
setkey(trainTrips.dt, song)
validTrips.hidden.dt <- fread("../Data/EvalDataYear1MSDWebsite//year1_valid_triplets_hidden.txt")
setnames(validTrips.hidden.dt, c("user", "song", "count"))
setkey(validTrips.hidden.dt, user)
kags <- validTrips.hidden.dt %>% group_by(user) %>% summarise(n())
setkey(kags, user)
songs.rated.base <- trainTrips.dt %>% group_by(song) %>% summarise(n.i.song = n(), score =zero(), score.cnt = zero())
setkey(songs.rated.base, song)
library(dplyr)
kags <- validTrips.hidden.dt %>% group_by(user) %>% summarise(n())
setkey(kags, user)
songs.rated.base <- trainTrips.dt %>% group_by(song) %>% summarise(n.i.song = n(), score =zero(), score.cnt = zero())
setkey(songs.rated.base, song)
songs.rated.base <- trainTrips.dt %>% group_by(song) %>% summarise(n.i.song = n())
setkey(songs.rated.base, song)
i = 1
u = kags[i, user]
actual <-  as.character(validTrips.hidden.dt[.(u),song] )
actual
getSongInfo <- function(s){
songInfo.dt(.(s), c(artist, song))
}
getSongInfo(actual)
getSongInfo <- function(s){
songInfo.dt[.(s), c(artist, song)]
}
getSongInfo(actual)
length(actual)
getSongInfo <- function(s){
songInfo.dt[.(s), c(artist, song)]
}
getSongInfo(actual)
getSongInfo <- function(s){
return(songInfo.dt[.(s), c(artist, song)])
}
getSongInfo(actual)
getSongInfo <- function(s){
return(songInfo.dt[.(s), .(artist, song)])
}
getSongInfo(actual)
sum(is.element(songs.rated.base$song, songInfo.dt$songid))
nrow(songs.rated.base)
which(!is.element(songs.rated.base$song, songInfo.dt$songid))
songs.rated.base[253255,]
all.artists <- unique(songInfo$artist)
all.artists <- unique(songInfo.dt$artist)
all.artists
length(all.artists)
songInfo.dt.sub <- songInfo.dt[is.element(songInfo.dt$songid, songs.rated.base$song),]
all.artists <-   unique(songInfo.dt.sub$artist)
all.artists
songInfo.dt <- songInfo.dt.sub
sum(is.element(songs.rated.base$song, songInfo.dt$songid))
table(is.element(songs.rated.base$song, songInfo.dt$songid))
save(all.artists, songInfo.dt, file = "../Data/songInfo.Rdata")
#input artist to get which tracks are available
getSongSubset <- function(a){
return(songInfo.dt[artist ==a, .(songid, artist, song)])
}
getSongSubset("Aura")
getSongSubset("Georgia")
getSongSubset("Giorgia")
getSongInfo( "SOUNOKS12A8C13BD0E")
runApp()
?shiny
library(shiny)
all.artists
runApp()
runApp()
getSongInfo( "SOUNOKS12A8C13BD0E")
getSongSubset("Georgia")
getSongSubset("Giorgia")
getSongSubset("Giorgia")$song
runApp()
